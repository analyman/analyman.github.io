{"meta":{"title":"DotCircle's Blog","subtitle":"","description":"","author":"DotCircle","url":"http://yoursite.com","root":"/"},"pages":[{"title":"About Me","date":"2020-09-23T05:30:40.959Z","updated":"2020-09-23T05:30:40.959Z","comments":true,"path":"about.html","permalink":"http://yoursite.com/about.html","excerpt":"","text":""}],"posts":[{"title":"Constraint Satisfication Problem","slug":"algorithm/constraint_satisfication_problem","date":"2020-09-23T05:30:40.959Z","updated":"2020-09-23T05:30:40.959Z","comments":true,"path":"2020/09/23/algorithm/constraint_satisfication_problem/","link":"","permalink":"http://yoursite.com/2020/09/23/algorithm/constraint_satisfication_problem/","excerpt":"","text":"定义约束满足问题","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://yoursite.com/categories/Algorithm/"}],"tags":[{"name":"CSP","slug":"CSP","permalink":"http://yoursite.com/tags/CSP/"}]},{"title":"Regularization","slug":"deep_learning/regularization","date":"2020-09-23T05:30:40.959Z","updated":"2020-09-23T05:30:40.959Z","comments":true,"path":"2020/09/23/deep_learning/regularization/","link":"","permalink":"http://yoursite.com/2020/09/23/deep_learning/regularization/","excerpt":"","text":"Hessian Matrix In mathematics, the Hessian matrix or Hessian is a square matrix of second-order partial derivatives of a scalar-valued function, or scalar field. It describes the local curvature of a function of many variables. The Hessian matrix was developed in the 19th century by the German mathematician Ludwig Otto Hesse and later named after him. Hesse originally used the term “functional determinants”. wikipedia,en.m.wikipedia.org/wiki/Hessian_matrix","categories":[{"name":"deep learning","slug":"deep-learning","permalink":"http://yoursite.com/categories/deep-learning/"}],"tags":[{"name":"regularization","slug":"regularization","permalink":"http://yoursite.com/tags/regularization/"},{"name":"deep learning","slug":"deep-learning","permalink":"http://yoursite.com/tags/deep-learning/"},{"name":"mean square error","slug":"mean-square-error","permalink":"http://yoursite.com/tags/mean-square-error/"}]},{"title":"Minix Clock","slug":"minix/kernel/clock","date":"2020-09-23T05:30:40.959Z","updated":"2020-09-23T05:30:40.959Z","comments":true,"path":"2020/09/23/minix/kernel/clock/","link":"","permalink":"http://yoursite.com/2020/09/23/minix/kernel/clock/","excerpt":"","text":"Clock files file brief description clock.h just some function declaration clock.c timers interrupt handler, load statistics update, functions to get statistics about times and set time lib/libtimers/timers_exp.c expire a minix_timer_t linked list lib/libtimers/timers_clr.c clear a minix timer in a linked list lib/libtimers/timers_set.c add a minix timer to a linked list types type where brief description minix_timer_t minix/timer.h timer with a callback, a sorted linked list tmr_func_t minix/timer.h timer callback function type, void (*)(int arg) struct kclockinfo type.h clock information 12345678struct kclockinfo &#123; time_t boottime; /* number of seconds since UNIX epoch */ clock_t uptime; /* number of clock ticks since system boot */ uint32_t _rsvd1; /* reserved for 64-bit uptime */ clock_t realtime; /* real time in clock ticks since boot */ uint32_t _rsvd2; /* reserved for 64-bit real time */ uint32_t hz; /* clock frequency in ticks per second */&#125;; variables variable where brief description kclockinfo glo.h global clock information kloadinfo aka loadinfo glo.h status of load average API function prototype where export brief description timer_int_handler int (*)(void) clock.c true handler of timer interrupt load_update void(*)(void) clock.c false update load information in kloadinfo, basicly number of ready process at this moment | timer_int_handler minix timer_int_handler The timer interrupt handler, and timer_int_handler() called from lapic_timer_int_handler which defined as 1234&#x2F;** arch&#x2F;i386&#x2F;apic_asm.S *&#x2F;&#x2F;* apic timer tick handlers *&#x2F;ENTRY(lapic_timer_int_handler) lapic_intr(_C_LABEL(timer_int_handler)) lapic_timer_int_handler is handler of APIC_TIMER_INT_VECTOR interrupt registered in apic_idt_init() function when kernel boot. 1234567891011121314/** arch/i386/apic.c *//* Build descriptors for interrupt gates in IDT. */void apic_idt_init(const int reset)&#123; is_bsp = is_boot_apic(apicid()); /* configure the timer interupt handler */ if (is_bsp) &#123; BOOT_VERBOSE(printf(&quot;Initiating APIC timer handler\\n&quot;)); /* register the timer interrupt handler for this CPU */ int_gate_idt(APIC_TIMER_INT_VECTOR, (vir_bytes) lapic_timer_int_handler, PRESENT | INT_GATE_TYPE | (INTR_PRIVILEGE &lt;&lt; DPL_SHIFT)); &#125;&#125; This handler increase a user tick of process of current processor and decrease virtual timer and profile timer of the process, if these timer expire after decreasing, raise corresponding signal to the process. The above processes also aply to bill process of current processor if this process is BILLABLE. If current processor is bootstraping processor, this handler will check clock_timer whether has expired, when true expire the timers. It also increase kclockinfo.realtime when current processor is bootstraping processor. load_update minix load_update store how many process is ready at this moment into kloadinfo, this information has history, keep a list of load at different moment(the interval may be very small).","categories":[{"name":"学习 Minix","slug":"学习-Minix","permalink":"http://yoursite.com/categories/%E5%AD%A6%E4%B9%A0-Minix/"}],"tags":[{"name":"minix","slug":"minix","permalink":"http://yoursite.com/tags/minix/"},{"name":"kernel","slug":"kernel","permalink":"http://yoursite.com/tags/kernel/"},{"name":"clock","slug":"clock","permalink":"http://yoursite.com/tags/clock/"}]},{"title":"Dimensionality Reduction","slug":"articlesnote/DimensionalityReduction","date":"2020-09-22T14:00:00.000Z","updated":"2020-09-23T05:30:40.959Z","comments":true,"path":"2020/09/22/articlesnote/DimensionalityReduction/","link":"","permalink":"http://yoursite.com/2020/09/22/articlesnote/DimensionalityReduction/","excerpt":"","text":"Dimensionality Reduction 简述 Dimensionality Reduction 即将高维的数据转化为较低维的数据, 当然这个变换要保持原有高维数据的一些特征才有意义。 通常高维度的数据因为Curse of dimensionality是比较稀疏(Sparse), 比较地难以处理地。 在某些情景下Dimensionality Reduction就会很有用, 当然前提是保持想要地特征。 Dimensionality Reduction的方法通常可以分为线性和非线性两种, 也可以分为Feature Selection和Feature Extraction。Dimensionality Reduction 可以用在诸如噪声消除(Noise reduction), 数据可视化(Data visualization), 聚类分析(Cluster analysis), 以及一些分析的中间过程。 Feature Selection Feature Extraction Principle Component Analysis 有一个$n$维线性空间中采样得到的$m$个样本, 记为数据集$X_{(m\\times n)}$。 PCA 用于求此数据集$X_{(m\\times n)}$的主成分(Principal Component aka. PC), 可以看作是求 “对数据达到最佳拟合的子空间 (lines and planes of closest fit to system of points in space)”。 而后可以用求得的主成分来改变原本数据的基, 这里可能只会用到值相对较大的 $PC$, 忽略那些值相对较小的 $PC$。 因为值相对小的成分有可能是数据中的噪声, 并没有价值。 PCA 在一些领域中有不同的名称, 如在数值分析中叫 Singular Value Decomposition (SVD), 在物理中被称为 特征向量分析(Eigenvector Analysis) 或者 特征分析(Characteristics Analysis), 还有一些其他学科中也有不同的名称。 分析 PCA 被定义为一个正交线性变换$V_{(n\\times n)}$, 其将数据变换到一个新的坐标系统。 设$x_i$为数据集的第$i$个样本即$X$的行向量, $v_j$为正交变换$V$的行向量即新坐标系统的第$j$个基, 那么可以得到第$i$个样本在新坐标系统中第$j$个基的坐标 si,j=xi⋅vjs_{i,j} = x_i\\cdot v_jsi,j​=xi​⋅vj​ 那么所有数据在$v_j$上的分量的平方和为$\\sum\\limits_{i=1}^ns_{i, j}^2$. 在此基础上可以认为所有样本数据在$n$维线性空间中最主要的方向为所有样本在 该方向的分量的平方和为最大, 设为$w$, 则有 w=arg⁡max⁡∣∣w∣∣2=1∑i=1n(xi⋅w)2=arg⁡max⁡∣∣w∣∣2=1∣∣Xw∣∣2=arg⁡max⁡∣∣w∣∣2=1wTXTXw\\begin{aligned} w &amp;= \\arg\\max\\limits_{\\left||w\\right||_2 = 1} \\sum\\limits_{i=1}^n\\left(x_i\\cdot w\\right)^2\\\\ &amp;= \\arg\\max\\limits_{||w||_2 = 1} ||Xw||_2\\\\ &amp;= \\arg\\max\\limits_{||w||_2 = 1} w^TX^TXw \\end{aligned}w​=arg∣∣w∣∣2​=1max​i=1∑n​(xi​⋅w)2=arg∣∣w∣∣2​=1max​∣∣Xw∣∣2​=arg∣∣w∣∣2​=1max​wTXTXw​ $X^TX$是一个$n\\times n$的对称矩阵, 并且是一个半正定矩阵。 以上的$w$为$X^TX$最大特征值对应的特征向量, 其可以用多种方式求得, 这里用 SVD 分解。 第二主要的方向可以通过减去最主要的方向的分量, 再利用上式求得。剩余的其他方向也是类似的。 Singluar Value Decomposition (SVD) 对任意一个矩阵$A_{(m\\times n)}$有以下分解 A=UΣVT=∑i=1rσiuiviTA = U\\Sigma V^T = \\sum\\limits_{i=1}^{r}\\sigma_iu_iv_i^TA=UΣVT=i=1∑r​σi​ui​viT​ $U$: $(m\\times m)$正交矩阵 $\\Sigma$: $(m\\times n)$对角矩阵 $V$: $(n\\times n)$正交矩阵 $r$: 矩阵$A$的秩 $u_i$: $U$的行向量 $v_i$: $V$的行向量 $\\sigma_i$: 对角矩阵的第$i$个对角分量, 此处可以设$i&gt;j\\Longrightarrow\\sigma_j\\ge\\sigma_i$ 其中$U$是$AA^T$的特征矩阵, $V$是$A^TA$的特征矩阵, 并且$\\mathrm{diag}(\\sigma_i^2)$为$AA^T$和$A^TA$的特征值。 SVD的推导与证明 设$A$的秩为$r$, 那么$AA^T$和$A^TA$的秩都为$r$(可以利用最大线性无关行向量组进行证明), $U_{(m\\times m)}$为$AA^T$的特征矩阵, $V_{(n\\times n)}$为$A^TA$的特征矩阵, 那么有 UTAATU=diag(pi)(i=1,…,m)U^TAA^TU = \\mathrm{diag}(p_i) \\qquad(i = 1, \\dots, m)UTAATU=diag(pi​)(i=1,…,m) 以及 VTATAV=diag(qi)(i=1,…,n)V^TA^TAV = \\mathrm{diag}(q_i) \\qquad(i = 1, \\dots, n)VTATAV=diag(qi​)(i=1,…,n) 这里$p_i$和$q_i$分别为$AA^T$和$A^TA$的特征值, 可以设为降序的。 设$U^TA = W_{(m\\times n)}$, 有 对$U$的第$i$个列向量$u_i$ ($i = 1,\\dots,r$), 那么$W$的第$i$个行向量有$w_i = u_i^TA$, 有$||w_i||_2 = \\sqrt{p_i}$, 设$h_i = {w_i\\over ||w_i||_2} = {w_i\\over \\sqrt{p_i}}$, 即 $h_i^T = {w_i\\over ||w_i||_2} = {u_i^TA\\over \\sqrt{p_i}}$, 下面证明$h_i^T$是$A^TA$的特征向量 所以$p_i$也是$A^TA$的特征值, $h_i$为对应$p_i$的特征向量, 并且可以得出$p_i = q_i,(i=1,\\dots,r)$。 设$V_{(n\\times n)}$的第$i$列向量为$v_i$, 那么当$i=1,\\dots,r$时, 有$v_i = h_i$。 设$\\sigma_i = \\sqrt{p_i}$, $\\Sigma_{(m\\times n)} = \\mathrm{diag}(\\sigma_1,\\dots,\\sigma_r)_{(m\\times n)}$ 当 $i=r+1,m$ 时, 有$u_i^TAA^Tu_i = 0$, 所以有$A^Tu_i = 0$, 同理可以得出$Av_i = 0$。 综上所述, 对于正交矩阵$U_{(m\\times m)}$, 正交矩阵$V_{(n\\times n)}$, 以及$\\Sigma_{(m\\times n)}$, 有 PCA的局限 Local Linearly Embedding Bibliography 1Svante WoldKim EsbensenPaul GeladiPrincial Component AnalysisElsevier Science Publishers19872WikipediaPrincipal Component Analysis3Sam RoweisLawrence SaulNonlinearly Dimensionality Reduction by Locally Linear EmbeddingScience20004Sam RoweisLawrence SaulAn Introduction to Locally Linear Embedding","categories":[{"name":"笔记","slug":"笔记","permalink":"http://yoursite.com/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"LLE","slug":"LLE","permalink":"http://yoursite.com/tags/LLE/"},{"name":"PCA","slug":"PCA","permalink":"http://yoursite.com/tags/PCA/"},{"name":"SVD","slug":"SVD","permalink":"http://yoursite.com/tags/SVD/"},{"name":"Dimensionality Reduction","slug":"Dimensionality-Reduction","permalink":"http://yoursite.com/tags/Dimensionality-Reduction/"}]}],"categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://yoursite.com/categories/Algorithm/"},{"name":"deep learning","slug":"deep-learning","permalink":"http://yoursite.com/categories/deep-learning/"},{"name":"学习 Minix","slug":"学习-Minix","permalink":"http://yoursite.com/categories/%E5%AD%A6%E4%B9%A0-Minix/"},{"name":"笔记","slug":"笔记","permalink":"http://yoursite.com/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"CSP","slug":"CSP","permalink":"http://yoursite.com/tags/CSP/"},{"name":"regularization","slug":"regularization","permalink":"http://yoursite.com/tags/regularization/"},{"name":"deep learning","slug":"deep-learning","permalink":"http://yoursite.com/tags/deep-learning/"},{"name":"mean square error","slug":"mean-square-error","permalink":"http://yoursite.com/tags/mean-square-error/"},{"name":"minix","slug":"minix","permalink":"http://yoursite.com/tags/minix/"},{"name":"kernel","slug":"kernel","permalink":"http://yoursite.com/tags/kernel/"},{"name":"clock","slug":"clock","permalink":"http://yoursite.com/tags/clock/"},{"name":"LLE","slug":"LLE","permalink":"http://yoursite.com/tags/LLE/"},{"name":"PCA","slug":"PCA","permalink":"http://yoursite.com/tags/PCA/"},{"name":"SVD","slug":"SVD","permalink":"http://yoursite.com/tags/SVD/"},{"name":"Dimensionality Reduction","slug":"Dimensionality-Reduction","permalink":"http://yoursite.com/tags/Dimensionality-Reduction/"}]}